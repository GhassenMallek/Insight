{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Insightpytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMuEyNt+jq8Wx74B2n9z63M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MGhassen30798/Insight/blob/Prediction-from-retina-pic-server/Insightpytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7sqq9QdEbRw",
        "outputId": "54141176-b799-45a7-ce24-cb17fd4f2ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import ast\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "#set global environments#\n",
        "\"\"\" the whole dataset used for reproduce the result can be downloaded via Googledrive: \n",
        "    for images(please download in a folder): https://drive.google.com/drive/folders/1cuCfvs5qR4BXxOk-_-ALzbZr5DdxsgXG?usp=sharing\n",
        "    for csv full_df.csv describing metadata: https://drive.google.com/file/d/1-XEKHT-EVWx2M-PmVYV1m4buBLW-QGqa/view?usp=sharing\"\"\"\n",
        "\n",
        "#Please fill in the absolute path of the full_df.csv file, including filename, e.g. \"/users/data/full_df.csv\"\n",
        "metadata_csv_path =  \"/content/drive/MyDrive/Ocular Disease/full_df.csv\"\n",
        "\n",
        "#Please fill in the absolute path where the image dataset is stored(downloaded via link shared from googledrive)\n",
        "#e.g. /users/data/images_directory\n",
        "image_path = \"/content/drive/MyDrive/Ocular Disease/preprocessed_images\"\n",
        "\n",
        "#num of classes to be predicted \n",
        "num_classes = 8\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 32\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 50\n",
        "\n",
        "\n",
        "#helper functions#\n",
        "#to undersample the data\n",
        "def undersample(dataframe, column= (), label = (), number=()):\n",
        "    df_undersampled = \"\"\n",
        "    df_m = dataframe\n",
        "    if isinstance(label, str):\n",
        "      df_presele = df_m.loc[df_m[column] == \"{}\".format(label)]\n",
        "      if len(df_presele) < number:\n",
        "        replace = True\n",
        "      else:\n",
        "        replace = False\n",
        "      df_sele = df_presele.sample(n=number, random_state = np.random.RandomState(),replace = replace)\n",
        "      df_non = df_m.loc[df_m[column] !=  \"{}\".format(label)]\n",
        "      df_undersampled = pd.concat([df_sele, df_non])\n",
        "    \n",
        "    if isinstance(label, list):\n",
        "        df_non = df_m.loc[~df_m[column].isin(label)]\n",
        "#     print(df_non)\n",
        "        for i,j in enumerate(label):\n",
        "            df_presele = df_m.loc[df_m[column] == j]\n",
        "            if len(df_presele)<number[i]:\n",
        "                replace = True\n",
        "            else:\n",
        "                replace = False\n",
        "#             print (number[i], j)\n",
        "        df_sele = df_presele.sample(n=number[i], random_state = np.random.RandomState(), replace = replace)\n",
        "        df_non = df_non.append(df_sele)\n",
        "        df_undersampled = df_non\n",
        "            \n",
        "    return df_undersampled\n",
        "\n",
        "#load the data#    \n",
        "label_aug = [\"red_contr\", \"rotated\", \"noised\"]\n",
        "df_path = metadata_csv_path #change the path to the dataframe that describes the metadata#\n",
        "df = pd.read_csv(df_path)\n",
        "\n",
        "df_data = df\n",
        "df_train, df_test = train_test_split(df_data, test_size=0.15, random_state=42, stratify = df_data.target)\n",
        "df_train.reset_index(drop = True)\n",
        "df_train = df_train.copy().reset_index(drop = True)\n",
        "\n",
        "# df_train_4aug = df_train.loc[df_train[\"target\"]!='[1, 0, 0, 0, 0, 0, 0, 0]']\n",
        "# df_train_norm = df_train.loc[df_train[\"target\"]=='[1, 0, 0, 0, 0, 0, 0, 0]']\n",
        "\n",
        "# df_aug_info = df_train.copy()\n",
        "\n",
        "# # for i in label_aug:\n",
        "# #     df_aug_i = df_train_4aug.copy()\n",
        "# #     df_aug_i[\"filename\"] = df_aug_i.apply(lambda x: i+\"_\"+df_aug_i[\"filename\"] , axis = 1)[1]\n",
        "# #     df_aug_info = pd.concat([df_aug_info, df_aug_i])\n",
        "\n",
        "\n",
        "# df_train_aug = undersample(df_aug_info, column=\"target\", label= [\"[0, 1, 0, 0, 0, 0, 0, 0]\", \"[0, 0, 0, 0, 0, 0, 0, 1]\",\n",
        "#                                                                 \"[0, 0, 0, 1, 0, 0, 0, 0]\", \"[0, 0, 1, 0, 0, 0, 0, 0]\", \"[0, 0, 0, 0, 1, 0, 0, 0]\",\n",
        "#                                                                 \"[0, 0, 0, 0, 0, 0, 1, 0]\", \"[0, 0, 0, 0, 0, 1, 0, 0]\"],\n",
        "#                          number = [2000]*7)\n",
        "# df_data_all  = pd.concat([df_train_aug, df_test])\n",
        "df_data_all=pd.concat([df_train, df_test])\n",
        "partition={}\n",
        "partition[\"train\"] = df_train.filename.tolist()\n",
        "partition[\"val\"]= df_test.filename.tolist()\n",
        "\n",
        "labels = {}\n",
        "for index, row in df_data_all.iterrows():\n",
        "    filename = row[\"filename\"]\n",
        "    labels[filename] = torch.tensor(np.array(ast.literal_eval(row[\"target\"])))\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataSet(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, main_dir, transform, list_IDs, labels):\n",
        "        'Initialization'\n",
        "        self.main_dir = main_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "      \n",
        "        return len(self.list_IDs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = self.list_IDs[index]\n",
        "\n",
        "        # Load data and get label\n",
        "        img_loc = os.path.join(self.main_dir, ID)\n",
        "        image = Image.open(img_loc).convert(\"RGB\")\n",
        "        tensor_image = self.transform(image)\n",
        "        label = self.labels[ID]\n",
        "\n",
        "        return tensor_image, label\n",
        "\n",
        "\n",
        "\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG13_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg13_bn(pretrained=False,num_classes=8)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3\n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs, model_name, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history   = []\n",
        "    train_acc_history = []\n",
        "    val_loss = []\n",
        "    train_loss = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.5\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels.type_as(outputs))\n",
        "                        loss2 = criterion(aux_outputs, labels.type_as(aux_outputs))\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        labels  = labels.type_as(outputs)\n",
        "                        loss = criterion(outputs, labels.type_as(outputs))\n",
        "\n",
        "                    _, preds = torch.max(outputs, dim = 1)\n",
        "                    _, trues = torch.max(labels, dim =1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    \n",
        "                    # print('Loss: {}'.format(loss)) \n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                \n",
        "\n",
        "                running_corrects += (preds == trues).sum()\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = np.float(running_corrects) / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, \n",
        "                                                       epoch_loss,\n",
        "                                                       epoch_acc\n",
        "                                                       ))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == \"train\":\n",
        "              train_acc_history.append(epoch_acc)\n",
        "              train_loss.append(epoch_loss)\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "              best_acc = epoch_acc\n",
        "              best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            if phase == 'val':\n",
        "              val_acc_history.append(epoch_acc)\n",
        "              val_loss.append(epoch_loss)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    \n",
        "    return model, train_acc_history, val_acc_history, train_loss, val_loss\n",
        "\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model_name = \"vgg\"\n",
        "feature_extract = False\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "model_ft.to(device)\n",
        "# Print the model we just instantiated\n",
        "# print(model_ft)\n",
        "\n",
        "image_path = image_path\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = image_path\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "\n",
        "# print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(params_to_update, lr=0.001, weight_decay=0.005)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "image_datasets = {x: CustomDataSet(data_dir, data_transforms[x], partition[x],labels) for x in [\"train\", \"val\"]}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "                                              batch_size=batch_size, shuffle=True, num_workers=2) for x in ['train','val']}\n",
        "\n",
        "\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist_train, hist_val, train_loss, val_loss = train_model(model_ft, dataloaders, criterion, optimizer, num_epochs, \"resnet\", is_inception=False)#hawni ici la bas\n",
        "df_acc_vgg = pd.DataFrame({\"train_acc\": hist_train, \"val_acc\":hist_val,\n",
        "                           \"train_loss\" : train_loss, \"val_loss\": val_loss})\n",
        "df_acc_vgg.to_csv(\"{}/{}.txt\".format(os.getcwd(), \"vgg_model_performance\"), sep = \"\\t\")\n",
        "print(\"training history is exported to {}\".format(os.getcwd()))\n",
        "preds = np.array([])\n",
        "vals  = np.array([])\n",
        "for input_bt, label_bt in dataloaders[\"val\"]:\n",
        "    input_bt = input_bt.to(device)\n",
        "    label_bt = label_bt.to(device)\n",
        "    y_preds = model_ft(input_bt)\n",
        "\n",
        "    _, pred_val = torch.max(y_preds, dim = 1)\n",
        "    _,true_val  = torch.max(label_bt, dim =1)\n",
        "\n",
        "    preds = np.append(preds, pred_val.cpu().numpy())\n",
        "    vals  = np.append(vals, true_val.cpu().numpy())\n",
        "\n",
        "\n",
        "classes = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
        "label_num = [0,1,2,3,4,5,6,7,]\n",
        "print(\"accuracy score for model is: {}\".format(accuracy_score(vals, preds)))\n",
        "print(\"f1_score for model is: {}\".format(f1_score(vals, preds, average = \"micro\")))\n",
        "confusion_matrix(vals, preds, labels = label_num)"
      ],
      "metadata": {
        "id": "mw8GGFAZ8Kyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5764df-f441-4caf-848a-ec204957d664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "\t features.0.weight\n",
            "\t features.0.bias\n",
            "\t features.1.weight\n",
            "\t features.1.bias\n",
            "\t features.3.weight\n",
            "\t features.3.bias\n",
            "\t features.4.weight\n",
            "\t features.4.bias\n",
            "\t features.7.weight\n",
            "\t features.7.bias\n",
            "\t features.8.weight\n",
            "\t features.8.bias\n",
            "\t features.10.weight\n",
            "\t features.10.bias\n",
            "\t features.11.weight\n",
            "\t features.11.bias\n",
            "\t features.14.weight\n",
            "\t features.14.bias\n",
            "\t features.15.weight\n",
            "\t features.15.bias\n",
            "\t features.17.weight\n",
            "\t features.17.bias\n",
            "\t features.18.weight\n",
            "\t features.18.bias\n",
            "\t features.21.weight\n",
            "\t features.21.bias\n",
            "\t features.22.weight\n",
            "\t features.22.bias\n",
            "\t features.24.weight\n",
            "\t features.24.bias\n",
            "\t features.25.weight\n",
            "\t features.25.bias\n",
            "\t features.28.weight\n",
            "\t features.28.bias\n",
            "\t features.29.weight\n",
            "\t features.29.bias\n",
            "\t features.31.weight\n",
            "\t features.31.bias\n",
            "\t features.32.weight\n",
            "\t features.32.bias\n",
            "\t classifier.0.weight\n",
            "\t classifier.0.bias\n",
            "\t classifier.3.weight\n",
            "\t classifier.3.bias\n",
            "\t classifier.6.weight\n",
            "\t classifier.6.bias\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.3595 Acc: 0.3970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:288: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.3077 Acc: 0.4494\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.3164 Acc: 0.4298\n",
            "val Loss: 0.3030 Acc: 0.4526\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3128 Acc: 0.4336\n",
            "val Loss: 0.3003 Acc: 0.4453\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.3110 Acc: 0.4373\n",
            "val Loss: 0.2981 Acc: 0.4505\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.3071 Acc: 0.4384\n",
            "val Loss: 0.2976 Acc: 0.4432\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.3048 Acc: 0.4373\n",
            "val Loss: 0.2958 Acc: 0.4473\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.3053 Acc: 0.4360\n",
            "val Loss: 0.2939 Acc: 0.4526\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.3028 Acc: 0.4351\n",
            "val Loss: 0.2940 Acc: 0.4453\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.3007 Acc: 0.4375\n",
            "val Loss: 0.2922 Acc: 0.4421\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.3002 Acc: 0.4386\n",
            "val Loss: 0.2919 Acc: 0.4494\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.2990 Acc: 0.4515\n",
            "val Loss: 0.2915 Acc: 0.4463\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.2975 Acc: 0.4493\n",
            "val Loss: 0.2901 Acc: 0.4473\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.2980 Acc: 0.4471\n",
            "val Loss: 0.2901 Acc: 0.4453\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.2947 Acc: 0.4447\n",
            "val Loss: 0.2893 Acc: 0.4432\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.2944 Acc: 0.4451\n",
            "val Loss: 0.2883 Acc: 0.4567\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.2930 Acc: 0.4460\n",
            "val Loss: 0.2876 Acc: 0.4463\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.2931 Acc: 0.4535\n",
            "val Loss: 0.2870 Acc: 0.4526\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.2920 Acc: 0.4500\n",
            "val Loss: 0.2869 Acc: 0.4536\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.2913 Acc: 0.4515\n",
            "val Loss: 0.2863 Acc: 0.4505\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.2902 Acc: 0.4456\n",
            "val Loss: 0.2851 Acc: 0.4546\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.2888 Acc: 0.4491\n",
            "val Loss: 0.2849 Acc: 0.4599\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.2892 Acc: 0.4557\n",
            "val Loss: 0.2848 Acc: 0.4567\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.2885 Acc: 0.4528\n",
            "val Loss: 0.2842 Acc: 0.4619\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.2872 Acc: 0.4625\n",
            "val Loss: 0.2840 Acc: 0.4526\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.2856 Acc: 0.4555\n",
            "val Loss: 0.2836 Acc: 0.4672\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.2854 Acc: 0.4589\n",
            "val Loss: 0.2833 Acc: 0.4599\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.2860 Acc: 0.4576\n",
            "val Loss: 0.2822 Acc: 0.4619\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.2854 Acc: 0.4611\n",
            "val Loss: 0.2819 Acc: 0.4661\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.2840 Acc: 0.4627\n",
            "val Loss: 0.2824 Acc: 0.4682\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.2841 Acc: 0.4648\n",
            "val Loss: 0.2817 Acc: 0.4692\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.2832 Acc: 0.4644\n",
            "val Loss: 0.2807 Acc: 0.4682\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.2826 Acc: 0.4636\n",
            "val Loss: 0.2806 Acc: 0.4609\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.2815 Acc: 0.4660\n",
            "val Loss: 0.2802 Acc: 0.4672\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.2812 Acc: 0.4690\n",
            "val Loss: 0.2802 Acc: 0.4672\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.2811 Acc: 0.4732\n",
            "val Loss: 0.2794 Acc: 0.4755\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.2806 Acc: 0.4675\n",
            "val Loss: 0.2797 Acc: 0.4619\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.2789 Acc: 0.4694\n",
            "val Loss: 0.2793 Acc: 0.4619\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.2796 Acc: 0.4679\n",
            "val Loss: 0.2791 Acc: 0.4734\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.2791 Acc: 0.4596\n",
            "val Loss: 0.2793 Acc: 0.4672\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.2775 Acc: 0.4756\n",
            "val Loss: 0.2786 Acc: 0.4651\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "train Loss: 0.2785 Acc: 0.4741\n",
            "val Loss: 0.2782 Acc: 0.4640\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "train Loss: 0.2763 Acc: 0.4767\n",
            "val Loss: 0.2780 Acc: 0.4640\n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "train Loss: 0.2763 Acc: 0.4763\n",
            "val Loss: 0.2774 Acc: 0.4724\n",
            "\n",
            "Epoch 43/49\n",
            "----------\n",
            "train Loss: 0.2761 Acc: 0.4811\n",
            "val Loss: 0.2773 Acc: 0.4755\n",
            "\n",
            "Epoch 44/49\n",
            "----------\n",
            "train Loss: 0.2749 Acc: 0.4806\n",
            "val Loss: 0.2770 Acc: 0.4713\n",
            "\n",
            "Epoch 45/49\n",
            "----------\n",
            "train Loss: 0.2754 Acc: 0.4775\n",
            "val Loss: 0.2771 Acc: 0.4640\n",
            "\n",
            "Epoch 46/49\n",
            "----------\n",
            "train Loss: 0.2745 Acc: 0.4837\n",
            "val Loss: 0.2763 Acc: 0.4651\n",
            "\n",
            "Epoch 47/49\n",
            "----------\n",
            "train Loss: 0.2738 Acc: 0.4762\n",
            "val Loss: 0.2762 Acc: 0.4692\n",
            "\n",
            "Epoch 48/49\n",
            "----------\n",
            "train Loss: 0.2740 Acc: 0.4848\n",
            "val Loss: 0.2756 Acc: 0.4682\n",
            "\n",
            "Epoch 49/49\n",
            "----------\n",
            "train Loss: 0.2741 Acc: 0.4830\n",
            "val Loss: 0.2755 Acc: 0.4734\n",
            "\n",
            "Training complete in 120m 13s\n",
            "Best val Acc: 0.500000\n",
            "training history is exported to /content\n",
            "accuracy score for model is: 0.07924921793534932\n",
            "f1_score for model is: 0.07924921793534932\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0, 202,   0,   0, 229],\n",
              "       [  0,   0,   0,   0, 126,   0,   0, 115],\n",
              "       [  0,   0,   0,   0,   9,   0,   0,  34],\n",
              "       [  0,   0,   0,   0,  17,   0,   0,  27],\n",
              "       [  0,   0,   0,   0,  18,   0,   0,  22],\n",
              "       [  0,   0,   0,   0,  10,   0,   0,   9],\n",
              "       [  0,   0,   0,   0,  16,   0,   0,  19],\n",
              "       [  0,   0,   0,   0,  48,   0,   0,  58]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_ft.state_dict(), '/content/drive/MyDrive/Ocular Disease/resnet.pth')"
      ],
      "metadata": {
        "id": "Dks6woSKYcup",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "528da910-8f21-496f-c9af-318e52ee7f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1c9a21c02f31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Ocular Disease/resnet.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import ast\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "#set global environments#\n",
        "\"\"\" the whole dataset used for reproduce the result can be downloaded via Googledrive: \n",
        "    for images(please download in a folder): https://drive.google.com/drive/folders/1cuCfvs5qR4BXxOk-_-ALzbZr5DdxsgXG?usp=sharing\n",
        "    for csv full_df.csv describing metadata: https://drive.google.com/file/d/1-XEKHT-EVWx2M-PmVYV1m4buBLW-QGqa/view?usp=sharing\"\"\"\n",
        "\n",
        "#Please fill in the absolute path of the full_df.csv file, including filename, e.g. \"/users/data/full_df.csv\"\n",
        "metadata_csv_path =  \"/content/drive/MyDrive/Ocular Disease/full_df.csv\"\n",
        "\n",
        "#Please fill in the absolute path where the image dataset is stored(downloaded via link shared from googledrive)\n",
        "#e.g. /users/data/images_directory\n",
        "image_path = \"/content/drive/MyDrive/Ocular Disease/preprocessed_images\"\n",
        "\n",
        "#num of classes to be predicted \n",
        "num_classes = 8\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 32\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 50\n",
        "\n",
        "\n",
        "#helper functions#\n",
        "#to undersample the data\n",
        "def undersample(dataframe, column= (), label = (), number=()):\n",
        "    df_undersampled = \"\"\n",
        "    df_m = dataframe\n",
        "    if isinstance(label, str):\n",
        "      df_presele = df_m.loc[df_m[column] == \"{}\".format(label)]\n",
        "      if len(df_presele) < number:\n",
        "        replace = True\n",
        "      else:\n",
        "        replace = False\n",
        "      df_sele = df_presele.sample(n=number, random_state = np.random.RandomState(),replace = replace)\n",
        "      df_non = df_m.loc[df_m[column] !=  \"{}\".format(label)]\n",
        "      df_undersampled = pd.concat([df_sele, df_non])\n",
        "    \n",
        "    if isinstance(label, list):\n",
        "        df_non = df_m.loc[~df_m[column].isin(label)]\n",
        "#     print(df_non)\n",
        "        for i,j in enumerate(label):\n",
        "            df_presele = df_m.loc[df_m[column] == j]\n",
        "            if len(df_presele)<number[i]:\n",
        "                replace = True\n",
        "            else:\n",
        "                replace = False\n",
        "#             print (number[i], j)\n",
        "        df_sele = df_presele.sample(n=number[i], random_state = np.random.RandomState(), replace = replace)\n",
        "        df_non = df_non.append(df_sele)\n",
        "        df_undersampled = df_non\n",
        "            \n",
        "    return df_undersampled\n",
        "\n",
        "#load the data#    \n",
        "label_aug = [\"red_contr\", \"rotated\", \"noised\"]\n",
        "df_path = metadata_csv_path #change the path to the dataframe that describes the metadata#\n",
        "df = pd.read_csv(df_path)\n",
        "\n",
        "df_data = df\n",
        "df_train, df_test = train_test_split(df_data, test_size=0.15, random_state=42, stratify = df_data.target)\n",
        "df_train.reset_index(drop = True)\n",
        "df_train = df_train.copy().reset_index(drop = True)\n",
        "\n",
        "# df_train_4aug = df_train.loc[df_train[\"target\"]!='[1, 0, 0, 0, 0, 0, 0, 0]']\n",
        "# df_train_norm = df_train.loc[df_train[\"target\"]=='[1, 0, 0, 0, 0, 0, 0, 0]']\n",
        "\n",
        "# df_aug_info = df_train.copy()\n",
        "\n",
        "# # for i in label_aug:\n",
        "# #     df_aug_i = df_train_4aug.copy()\n",
        "# #     df_aug_i[\"filename\"] = df_aug_i.apply(lambda x: i+\"_\"+df_aug_i[\"filename\"] , axis = 1)[1]\n",
        "# #     df_aug_info = pd.concat([df_aug_info, df_aug_i])\n",
        "\n",
        "\n",
        "# df_train_aug = undersample(df_aug_info, column=\"target\", label= [\"[0, 1, 0, 0, 0, 0, 0, 0]\", \"[0, 0, 0, 0, 0, 0, 0, 1]\",\n",
        "#                                                                 \"[0, 0, 0, 1, 0, 0, 0, 0]\", \"[0, 0, 1, 0, 0, 0, 0, 0]\", \"[0, 0, 0, 0, 1, 0, 0, 0]\",\n",
        "#                                                                 \"[0, 0, 0, 0, 0, 0, 1, 0]\", \"[0, 0, 0, 0, 0, 1, 0, 0]\"],\n",
        "#                          number = [2000]*7)\n",
        "# df_data_all  = pd.concat([df_train_aug, df_test])\n",
        "df_data_all=pd.concat([df_train, df_test])\n",
        "partition={}\n",
        "partition[\"train\"] = df_train.filename.tolist()\n",
        "partition[\"val\"]= df_test.filename.tolist()\n",
        "\n",
        "labels = {}\n",
        "for index, row in df_data_all.iterrows():\n",
        "    filename = row[\"filename\"]\n",
        "    labels[filename] = torch.tensor(np.array(ast.literal_eval(row[\"target\"])))\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataSet(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, main_dir, transform, list_IDs, labels):\n",
        "        'Initialization'\n",
        "        self.main_dir = main_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "      \n",
        "        return len(self.list_IDs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = self.list_IDs[index]\n",
        "\n",
        "        # Load data and get label\n",
        "        img_loc = os.path.join(self.main_dir, ID)\n",
        "        image = Image.open(img_loc).convert(\"RGB\")\n",
        "        tensor_image = self.transform(image)\n",
        "        label = self.labels[ID]\n",
        "\n",
        "        return tensor_image, label\n",
        "\n",
        "\n",
        "\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG13_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg13_bn(pretrained=False,num_classes=8)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3\n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs, model_name, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history   = []\n",
        "    train_acc_history = []\n",
        "    val_loss = []\n",
        "    train_loss = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.5\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels.type_as(outputs))\n",
        "                        loss2 = criterion(aux_outputs, labels.type_as(aux_outputs))\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        labels  = labels.type_as(outputs)\n",
        "                        loss = criterion(outputs, labels.type_as(outputs))\n",
        "\n",
        "                    _, preds = torch.max(outputs, dim = 1)\n",
        "                    _, trues = torch.max(labels, dim =1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    \n",
        "                    # print('Loss: {}'.format(loss)) \n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                \n",
        "\n",
        "                running_corrects += (preds == trues).sum()\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = np.float(running_corrects) / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, \n",
        "                                                       epoch_loss,\n",
        "                                                       epoch_acc\n",
        "                                                       ))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == \"train\":\n",
        "              train_acc_history.append(epoch_acc)\n",
        "              train_loss.append(epoch_loss)\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "              best_acc = epoch_acc\n",
        "              best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            if phase == 'val':\n",
        "              val_acc_history.append(epoch_acc)\n",
        "              val_loss.append(epoch_loss)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    \n",
        "    return model, train_acc_history, val_acc_history, train_loss, val_loss\n",
        "\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model_name = \"vgg\"\n",
        "feature_extract = False\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "model_ft.to(device)\n",
        "# Print the model we just instantiated\n",
        "# print(model_ft)\n",
        "\n",
        "image_path = image_path\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = image_path\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "\n",
        "# print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(params_to_update, lr=0.001, weight_decay=0.005)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "image_datasets = {x: CustomDataSet(data_dir, data_transforms[x], partition[x],labels) for x in [\"train\", \"val\"]}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "                                              batch_size=batch_size, shuffle=True, num_workers=2) for x in ['train','val']}\n",
        "\n",
        "\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist_train, hist_val, train_loss, val_loss = train_model(model_ft, dataloaders, criterion, optimizer, num_epochs, \"inception\", is_inception=False)#hawni ici la bas\n",
        "df_acc_vgg = pd.DataFrame({\"train_acc\": hist_train, \"val_acc\":hist_val,\n",
        "                           \"train_loss\" : train_loss, \"val_loss\": val_loss})\n",
        "df_acc_vgg.to_csv(\"{}/{}.txt\".format(os.getcwd(), \"vgg_model_performance\"), sep = \"\\t\")\n",
        "print(\"training history is exported to {}\".format(os.getcwd()))\n",
        "preds = np.array([])\n",
        "vals  = np.array([])\n",
        "for input_bt, label_bt in dataloaders[\"val\"]:\n",
        "    input_bt = input_bt.to(device)\n",
        "    label_bt = label_bt.to(device)\n",
        "    y_preds = model_ft(input_bt)\n",
        "\n",
        "    _, pred_val = torch.max(y_preds, dim = 1)\n",
        "    _,true_val  = torch.max(label_bt, dim =1)\n",
        "\n",
        "    preds = np.append(preds, pred_val.cpu().numpy())\n",
        "    vals  = np.append(vals, true_val.cpu().numpy())\n",
        "\n",
        "\n",
        "classes = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
        "label_num = [0,1,2,3,4,5,6,7,]\n",
        "print(\"accuracy score for model is: {}\".format(accuracy_score(vals, preds)))\n",
        "print(\"f1_score for model is: {}\".format(f1_score(vals, preds, average = \"micro\")))\n",
        "confusion_matrix(vals, preds, labels = label_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiAwD8M5J94q",
        "outputId": "51aa42bd-cc9c-4cc1-ca80-207ea7131916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "\t features.0.weight\n",
            "\t features.0.bias\n",
            "\t features.1.weight\n",
            "\t features.1.bias\n",
            "\t features.3.weight\n",
            "\t features.3.bias\n",
            "\t features.4.weight\n",
            "\t features.4.bias\n",
            "\t features.7.weight\n",
            "\t features.7.bias\n",
            "\t features.8.weight\n",
            "\t features.8.bias\n",
            "\t features.10.weight\n",
            "\t features.10.bias\n",
            "\t features.11.weight\n",
            "\t features.11.bias\n",
            "\t features.14.weight\n",
            "\t features.14.bias\n",
            "\t features.15.weight\n",
            "\t features.15.bias\n",
            "\t features.17.weight\n",
            "\t features.17.bias\n",
            "\t features.18.weight\n",
            "\t features.18.bias\n",
            "\t features.21.weight\n",
            "\t features.21.bias\n",
            "\t features.22.weight\n",
            "\t features.22.bias\n",
            "\t features.24.weight\n",
            "\t features.24.bias\n",
            "\t features.25.weight\n",
            "\t features.25.bias\n",
            "\t features.28.weight\n",
            "\t features.28.bias\n",
            "\t features.29.weight\n",
            "\t features.29.bias\n",
            "\t features.31.weight\n",
            "\t features.31.bias\n",
            "\t features.32.weight\n",
            "\t features.32.bias\n",
            "\t classifier.0.weight\n",
            "\t classifier.0.bias\n",
            "\t classifier.3.weight\n",
            "\t classifier.3.bias\n",
            "\t classifier.6.weight\n",
            "\t classifier.6.bias\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.3614 Acc: 0.3989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:288: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.3115 Acc: 0.4484\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.3176 Acc: 0.4333\n",
            "val Loss: 0.3049 Acc: 0.4463\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3148 Acc: 0.4309\n",
            "val Loss: 0.3022 Acc: 0.4473\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.3113 Acc: 0.4344\n",
            "val Loss: 0.2990 Acc: 0.4494\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.3084 Acc: 0.4421\n",
            "val Loss: 0.2973 Acc: 0.4536\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.3054 Acc: 0.4438\n",
            "val Loss: 0.2955 Acc: 0.4526\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.3028 Acc: 0.4408\n",
            "val Loss: 0.2943 Acc: 0.4432\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.3016 Acc: 0.4382\n",
            "val Loss: 0.2935 Acc: 0.4484\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.3005 Acc: 0.4436\n",
            "val Loss: 0.2921 Acc: 0.4515\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.2994 Acc: 0.4454\n",
            "val Loss: 0.2912 Acc: 0.4453\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.2981 Acc: 0.4458\n",
            "val Loss: 0.2908 Acc: 0.4484\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.2978 Acc: 0.4482\n",
            "val Loss: 0.2902 Acc: 0.4380\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.2956 Acc: 0.4460\n",
            "val Loss: 0.2890 Acc: 0.4432\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.2953 Acc: 0.4447\n",
            "val Loss: 0.2879 Acc: 0.4453\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.2933 Acc: 0.4493\n",
            "val Loss: 0.2870 Acc: 0.4515\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.2918 Acc: 0.4574\n",
            "val Loss: 0.2865 Acc: 0.4515\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.2919 Acc: 0.4530\n",
            "val Loss: 0.2864 Acc: 0.4494\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.2905 Acc: 0.4600\n",
            "val Loss: 0.2855 Acc: 0.4578\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.2893 Acc: 0.4555\n",
            "val Loss: 0.2850 Acc: 0.4526\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.2890 Acc: 0.4594\n",
            "val Loss: 0.2850 Acc: 0.4588\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.2873 Acc: 0.4596\n",
            "val Loss: 0.2849 Acc: 0.4557\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.2866 Acc: 0.4622\n",
            "val Loss: 0.2836 Acc: 0.4546\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.2860 Acc: 0.4614\n",
            "val Loss: 0.2830 Acc: 0.4557\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.2858 Acc: 0.4642\n",
            "val Loss: 0.2828 Acc: 0.4578\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.2842 Acc: 0.4754\n",
            "val Loss: 0.2827 Acc: 0.4609\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.2844 Acc: 0.4670\n",
            "val Loss: 0.2819 Acc: 0.4609\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.2846 Acc: 0.4620\n",
            "val Loss: 0.2818 Acc: 0.4682\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.2833 Acc: 0.4636\n",
            "val Loss: 0.2801 Acc: 0.4630\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.2826 Acc: 0.4714\n",
            "val Loss: 0.2807 Acc: 0.4692\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.2812 Acc: 0.4684\n",
            "val Loss: 0.2813 Acc: 0.4536\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.2815 Acc: 0.4745\n",
            "val Loss: 0.2800 Acc: 0.4672\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.2807 Acc: 0.4749\n",
            "val Loss: 0.2797 Acc: 0.4672\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.2798 Acc: 0.4756\n",
            "val Loss: 0.2802 Acc: 0.4578\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.2797 Acc: 0.4697\n",
            "val Loss: 0.2793 Acc: 0.4713\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.2791 Acc: 0.4708\n",
            "val Loss: 0.2785 Acc: 0.4672\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.2785 Acc: 0.4762\n",
            "val Loss: 0.2787 Acc: 0.4672\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.2764 Acc: 0.4729\n",
            "val Loss: 0.2785 Acc: 0.4724\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.2762 Acc: 0.4765\n",
            "val Loss: 0.2782 Acc: 0.4661\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.2772 Acc: 0.4776\n",
            "val Loss: 0.2778 Acc: 0.4640\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.2762 Acc: 0.4769\n",
            "val Loss: 0.2777 Acc: 0.4630\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "train Loss: 0.2756 Acc: 0.4797\n",
            "val Loss: 0.2764 Acc: 0.4745\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "train Loss: 0.2754 Acc: 0.4771\n",
            "val Loss: 0.2769 Acc: 0.4672\n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "train Loss: 0.2747 Acc: 0.4752\n",
            "val Loss: 0.2760 Acc: 0.4724\n",
            "\n",
            "Epoch 43/49\n",
            "----------\n",
            "train Loss: 0.2752 Acc: 0.4786\n",
            "val Loss: 0.2759 Acc: 0.4755\n",
            "\n",
            "Epoch 44/49\n",
            "----------\n",
            "train Loss: 0.2740 Acc: 0.4824\n",
            "val Loss: 0.2757 Acc: 0.4703\n",
            "\n",
            "Epoch 45/49\n",
            "----------\n",
            "train Loss: 0.2729 Acc: 0.4784\n",
            "val Loss: 0.2759 Acc: 0.4619\n",
            "\n",
            "Epoch 46/49\n",
            "----------\n",
            "train Loss: 0.2745 Acc: 0.4771\n",
            "val Loss: 0.2752 Acc: 0.4692\n",
            "\n",
            "Epoch 47/49\n",
            "----------\n",
            "train Loss: 0.2731 Acc: 0.4824\n",
            "val Loss: 0.2752 Acc: 0.4776\n",
            "\n",
            "Epoch 48/49\n",
            "----------\n",
            "train Loss: 0.2722 Acc: 0.4791\n",
            "val Loss: 0.2751 Acc: 0.4765\n",
            "\n",
            "Epoch 49/49\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_ft.state_dict(), '/content/drive/MyDrive/Ocular Disease/inception.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "cguOgPYUKV9m",
        "outputId": "34abc5e4-1235-403c-c41d-80d1cf4ed729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0c622ccc2487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Ocular Disease/inception.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    }
  ]
}